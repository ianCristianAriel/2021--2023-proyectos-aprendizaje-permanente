{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ec11e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.item import Field\n",
    "from scrapy.item import Item\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.loader.processors import MapCompose\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.loader import ItemLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48bab9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino una abstraccion para cada tipo de informacion que quiero extraer\n",
    "# Cada una tiene sus propias propiedades diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0704c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Articulo(Item):\n",
    "    titulo = Field()\n",
    "    contenido = Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd6822a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Review(Item):\n",
    "    titulo = Field()\n",
    "    calificacion = Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7261e542",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Video(Item):\n",
    "    titulo = Field()\n",
    "    fecha_de_publicacion = Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd5c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASE CORE - Al querer hacer extraccion de multiples paginas, heredamos de CrawlSpider\n",
    "class TripAdvisor(CrawlSpider):\n",
    "    name = 'hotelestripadvisor'\n",
    "    custom_settings = {\n",
    "        'USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/71.0.3578.80 Chrome/71.0.3578.80 Safari/537.36',\n",
    "        # Como ordenar las columnas en el CSV?\n",
    "        'FEED_EXPORT_FIELDS': ['id', 'descripcion', 'titular'],\n",
    "        'CONCURRENT_REQUESTS': 1,  # numero de requerimientos concurrentes\n",
    "        # Numero maximo de paginas en las cuales voy a descargar items. Scrapy se cierra cuando alcanza este numero\n",
    "        'CLOSESPIDER_PAGECOUNT': 20,\n",
    "        'FEED_EXPORT_ENCODING': 'utf-8',  # Tipo de codificacion del archivo de salida\n",
    "        # En caso de utilizar algun crawler en la nuve como crawlera\n",
    "        'DOWNLOADER_MIDDLEWARES': {'scrapy_crawlera.CrawleraMiddleware': 610},\n",
    "        'CRAWLERA_ENABLED': True,\n",
    "        'CRAWLERA_APIKEY': 'INGRESA_TU_API_KEY',\n",
    "    }\n",
    "\n",
    "    # Reduce el espectro de busqueda de URLs. No nos podemos salir de los dominios de esta lista\n",
    "    # Utilizamos 2 dominios permitidos, ya que los articulos utilizan un dominio diferente\n",
    "    allowed_domains = ['articulo.mercadolibre.com.ec',\n",
    "                       'listado.mercadolibre.com.ec']\n",
    "\n",
    "    # Url semilla a la cual se hara el primer requerimiento\n",
    "    start_urls = [\n",
    "        'https://www.tripadvisor.com/Hotels-g303845-Guayaquil_Guayas_Province-Hotels.html']\n",
    "\n",
    "    # Tiempo de espera entre cada requerimiento. Nos ayuda a proteger nuestra IP.\n",
    "    # No va a ser dos, va a ser 0.5 * download_delay hasta 1.5 * download delay\n",
    "    # es decir, va a ser entre 1 y 3 segundos de una manera randomica. Ya es un comportamiento por defecto\n",
    "    download_delay = 2\n",
    "\n",
    "    # Tupla de reglas para direccionar el movimiento de nuestro Crawler a traves de las paginas\n",
    "    rules = (\n",
    "        Rule(  # Regla de movimiento VERTICAL hacia el detalle de los hoteles\n",
    "            LinkExtractor(\n",
    "                allow=r'/Hotel_Review-'  # Si la URL contiene este patron, haz un requerimiento a esa URL\n",
    "            ), follow=True, callback=\"parse_hotel\"),  # El callback es el nombre de la funcion que se va a llamar con la respuesta al requerimiento hacia estas URLs y extraer datos\n",
    "    )\n",
    "\n",
    "    # Funcion a utilizar con MapCompose para realizar limpieza de datos\n",
    "    def quitarDolar(self, texto):\n",
    "        return texto.replace(\"$\", \"\")\n",
    "\n",
    "    # EL RESPONSE ES EL DE LA URL SEMILLA\n",
    "    rules = (\n",
    "        Rule(\n",
    "            LinkExtractor(\n",
    "                allow=r'type='\n",
    "            ), follow=True),  # HORIZONTALIDAD POR TIPO => No tiene callback ya que aqui no voy a extraer datos\n",
    "        Rule(LinkExtractor(\n",
    "            allow=r'&page=\\d+'\n",
    "        ), follow=True),  # HORIZONTALIDAD DE PAGINACION EN CADA TIPO => No tiene callback ya que aqui no voy a extraer datos\n",
    "\n",
    "        # Una regla por cada tipo de contenido donde ire verticalmente\n",
    "        # Cada una tiene su propia funcion parse que extraera los items dependiendo de la estructura del HTML donde esta cada tipo de item\n",
    "        Rule(\n",
    "            LinkExtractor(  # VERTICALIDAD DE REVIEWS\n",
    "                allow=r'/review/'\n",
    "            ), follow=True, callback='parse_review'),\n",
    "        Rule(\n",
    "            LinkExtractor(  # VERTICALIDAD DE VIDEOS\n",
    "                allow=r'/video/'\n",
    "            ), follow=True, callback='parse_video'),\n",
    "        Rule(\n",
    "            LinkExtractor(\n",
    "                allow=r'/news/'  # VERTICALIDAD DE ARTICULOS\n",
    "            ), follow=True, callback='parse_news'),\n",
    "        Rule(\n",
    "            LinkExtractor(\n",
    "                allow=r'start=',\n",
    "                # Si los lincks se encuentran en tagas distintos al \"a\"\n",
    "                tags=('a', 'button'),\n",
    "                # Si los links anteriores no llevan un \"href\", se coloca el que lleva (ej: data-url)\n",
    "                attrs=('href', 'data-url')\n",
    "            ), follow=True, callback=\"parse_farmacia\"),\n",
    "    )\n",
    "\n",
    "    # DEFINICION DE CADA FUNCION PARSEADORA DE CADA TIPO DE INFORMACION\n",
    "\n",
    "    # REVIEW\n",
    "    def parse_review(self, response):\n",
    "        item = ItemLoader(Review(), response)\n",
    "        item.add_xpath('titulo', '//h1/text()')\n",
    "        item.add_xpath(\n",
    "            'calificacion', '//span[@class=\"side-wrapper side-wrapper hexagon-content\"]/text()')\n",
    "        yield item.load_item()\n",
    "\n",
    "    # VIDEO\n",
    "    def parse_video(self, response):\n",
    "        item = ItemLoader(Video(), response)\n",
    "        item.add_xpath('titulo', '//h1/text()')\n",
    "        item.add_xpath('fecha_de_publicacion',\n",
    "                       '//span[@class=\"publish-date\"]/text()')\n",
    "        yield item.load_item()\n",
    "\n",
    "    # ARTICULO\n",
    "    def parse_news(self, response):\n",
    "        item = ItemLoader(Articulo(), response)\n",
    "        item.add_xpath('titulo', '//h1/text()')\n",
    "        item.add_xpath('contenido', '//div[@id=\"id_text\"]//*/text()')\n",
    "        yield item.load_item()\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Selectores: Clase de scrapy para extraer datos\n",
    "        sel = Selector(response)\n",
    "        # El selector se utiliza cuando se quiere tomar un tag html padre y formar una lista cons us tags hijos\n",
    "        preguntas = sel.xpath(\n",
    "            '//div[@id=\"questions\"]//div[@class=\"question-summary\"]')\n",
    "        i = 0\n",
    "        for pregunta in preguntas:\n",
    "            # Instancio mi ITEM con el selector en donde estan los datos para llenarlo\n",
    "            item = ItemLoader(Pregunta(), pregunta)\n",
    "\n",
    "            # Lleno las propiedades de mi ITEM a traves de expresiones XPATH a buscar dentro del selector \"pregunta\"\n",
    "            item.add_xpath('pregunta', './/h3/a/text()')\n",
    "            item.add_xpath('descripcion', './/div[@class=\"excerpt\"]/text()')\n",
    "            item.add_value('id', i)\n",
    "            i += 1\n",
    "            # Hago Yield de la informacion para que se escriban los datos en el archivo\n",
    "            yield item.load_item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb90cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJECUCION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205cc1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por terminal: \n",
    "# scrapy runspider 4_eluniverso.py -o resultados.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90147e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Â CORRIENDO SCRAPY SIN LA TERMINAL\n",
    "\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'datos_de_salida.json'\n",
    "})\n",
    "process.crawl(ElUniversoSpider)\n",
    "process.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
